     Home Depot, similar to many other retail stores, relies heavily on online sales to succeed in today’s connected business world. Connecting online customers to the products that they desire quickly and efficiently is vital to a successful online business. Search result relevance is a key metric of Home Depot’s success in this endeavor. Thus, Home Depot has challenged Kagglers to build a model that accurately predicts the relevance of search results. Currently, search result relevancy scores are improved via human rating, which is a slow and rather subjective process. 
     Home Depot has provided training and testing datasets and detailed product information for Kagglers to use to build their models. The training dataset is composed of search terms, product names, and relevance scores. Detailed product information and product attributes can be joined from a separate dataset. Currently, I do not plan to supplement the available data with any additional data.
     Instead of vectorizing the text and building a sparse matrix, which can then be used to build a model, I plan to use functions to create many different numerical features, which provide descriptive and quantitative information about each search term and result pairing. For example, Levenshtein’s Edit Distance is the count of changes required to convert string A into string B, and I will calculate the Levenshtein Edit Distance to convert a search term into a product name. As another example of a feature, I will count the number of occurrences of each search term word in the product name and product description. This method will build a train and test dataset where the number of rows is much greater than the number of features and leave more supervised learning methods available to me. Depending on the effectiveness of various stemming programs that I try, I may need to use regular expression to build a stemming function that is highly specific to this dataset.
     At the end of this project, I plan to deliver a Jupyter Notebook of summary statistics, a Jupyter Notebook of code to process the datasets and build my model, and a slide deck detailing the project. 
     

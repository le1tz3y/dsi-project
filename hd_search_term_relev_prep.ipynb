{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import time\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#from sklearn import cross_validation\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "#from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#from nltk.stem import porter\n",
    "#from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Files Loaded: 0.07 minutes ---\n"
     ]
    }
   ],
   "source": [
    "# Start file load timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Import all dataframes and build the train dataframe\n",
    "df_train = pd.DataFrame.from_csv('train.csv', index_col=None, encoding=\"ISO-8859-1\")\n",
    "df_test = pd.DataFrame.from_csv('test.csv', index_col=None, encoding=\"ISO-8859-1\")\n",
    "df_prod_desc = pd.DataFrame.from_csv('product_descriptions.csv', index_col=None, encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Add Brand column/feature, but a lot of rows are NaN\n",
    "df_attrib = pd.DataFrame.from_csv('attributes.csv', index_col=None, encoding=\"ISO-8859-1\")\n",
    "df_brand = df_attrib[df_attrib.name == \"MFG Brand Name\"][[\"product_uid\", \"value\"]].rename(columns={\"value\": \"brand\"})\n",
    "\n",
    "# Find the length of the train dataset to use for train/test split that occurs later\n",
    "num_train = df_train.shape[0]\n",
    "\n",
    "# Concat train and test datasets before cleaning data\n",
    "df = pd.concat((df_train, df_test), axis=0, ignore_index=True)\n",
    "\n",
    "# Add prod_desc and brand information\n",
    "df = pd.merge(df, df_prod_desc, how='left', on='product_uid')\n",
    "df = pd.merge(df, df_brand, how='left', on='product_uid')\n",
    "\n",
    "print(\"--- Files Loaded: %s minutes ---\" % round(((time.time() - start_time)/60),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define necessary variables and functions\n",
    "stemmer = porter.PorterStemmer()\n",
    "stop_w = ['for', 'xbi', 'and', 'in', 'th','on','sku','with','what','from','that','less','er','ing'] #'electr','paint','pipe','light','kitchen','wood','outdoor','door','bathroom'\n",
    "strNum = {'zero':0,'one':1,'two':2,'three':3,'four':4,'five':5,'six':6,'seven':7,'eight':8,'nine':9}\n",
    "\n",
    "def str_stem(s):\n",
    "    s = unicodedata.normalize('NFD', unicode(s)).encode('ascii', 'ignore')\n",
    "    s = re.sub(r\"(\\w)\\.([A-Z])\", r\"\\1 \\2\", s) #Split words with a.A\n",
    "    s = s.lower()\n",
    "    s = s.replace(\"  \", \" \")\n",
    "    s = s.replace(\",\", \"\") #could be number / segment later\n",
    "    s = s.replace(\"$\", \" \")\n",
    "    s = s.replace(\"?\", \" \")\n",
    "    s = s.replace(\"-\", \" \")\n",
    "    s = s.replace(\"//\", \"/\")\n",
    "    s = s.replace(\"..\", \".\")\n",
    "    s = s.replace(\" / \", \" \")\n",
    "    s = s.replace(\" \\\\ \", \" \")\n",
    "    s = s.replace(\".\", \" . \")\n",
    "    s = re.sub(r\"(^\\.|/)\", r\"\", s)\n",
    "    s = re.sub(r\"(\\.|/)$\", r\"\", s)\n",
    "    s = re.sub(r\"([0-9])([a-z])\", r\"\\1 \\2\", s)\n",
    "    s = re.sub(r\"([a-z])([0-9])\", r\"\\1 \\2\", s)\n",
    "    s = s.replace(\" x \", \" xbi \")\n",
    "    s = re.sub(r\"([a-z])( *)\\.( *)([a-z])\", r\"\\1 \\4\", s)\n",
    "    s = re.sub(r\"([a-z])( *)/( *)([a-z])\", r\"\\1 \\4\", s)\n",
    "    s = s.replace(\"*\", \" xbi \")\n",
    "    s = s.replace(\" by \", \" xbi \")\n",
    "    s = re.sub(r\"([0-9])( *)\\.( *)([0-9])\", r\"\\1.\\4\", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(inches|inch|in|')\\.?\", r\"\\1in. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(foot|feet|ft|'')\\.?\", r\"\\1ft. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(pounds|pound|lbs|lb)\\.?\", r\"\\1lb. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(square|sq) ?\\.?(feet|foot|ft)\\.?\", r\"\\1sq.ft. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(cubic|cu) ?\\.?(feet|foot|ft)\\.?\", r\"\\1cu.ft. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(gallons|gallon|gal)\\.?\", r\"\\1gal. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(ounces|ounce|oz)\\.?\", r\"\\1oz. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(centimeters|cm)\\.?\", r\"\\1cm. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(milimeters|mm)\\.?\", r\"\\1mm. \", s)\n",
    "    s = s.replace(\"Â°\", \" degrees \")\n",
    "    s = re.sub(r\"([0-9]+)( *)(degrees|degree)\\.?\", r\"\\1deg. \", s)\n",
    "    s = s.replace(\" v \", \" volts \")\n",
    "    s = re.sub(r\"([0-9]+)( *)(volts|volt)\\.?\", r\"\\1volt. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(watts|watt)\\.?\", r\"\\1watt. \", s)\n",
    "    s = re.sub(r\"([0-9]+)( *)(amperes|ampere|amps|amp)\\.?\", r\"\\1amp. \", s)\n",
    "    s = s.replace(\"  \", \" \")\n",
    "    s = s.replace(\" . \", \" \")\n",
    "    s = (\" \").join([str(strNum[z]) if z in strNum else z for z in s.split(\" \")])\n",
    "    s = (\" \").join([stemmer.stem(z) for z in s.split(\" \")])\n",
    "    s = s.lower()\n",
    "    s = s.replace(\"toliet\", \"toilet\")\n",
    "    s = s.replace(\"airconditioner\", \"air conditioner\")\n",
    "    s = s.replace(\"vinal\", \"vinyl\")\n",
    "    s = s.replace(\"vynal\", \"vinyl\")\n",
    "    s = s.replace(\"skill\", \"skil\")\n",
    "    s = s.replace(\"snowbl\", \"snow bl\")\n",
    "    s = s.replace(\"plexigla\", \"plexi gla\")\n",
    "    s = s.replace(\"rustoleum\", \"rust oleum\")\n",
    "    s = s.replace(\"whirpool\", \"whirlpool\")\n",
    "    s = s.replace(\"whirlpoolga\", \"whirlpool ga\")\n",
    "    s = s.replace(\"whirlpoolstainless\", \"whirlpool stainless\")\n",
    "    return s\n",
    "\n",
    "\n",
    "# Depends on segmentit\n",
    "def seg_words(str1, str2):\n",
    "    str2 = str2.lower()\n",
    "    str2 = re.sub(\"[^a-z0-9./]\", \" \", str2)\n",
    "    str2 = [z for z in set(str2.split()) if len(z) > 2]\n",
    "    words = str1.lower().split(\" \")\n",
    "    s9 = []\n",
    "    for word in words:\n",
    "        if len(word) > 3:\n",
    "            s1 = []\n",
    "            s1 += segmentit(word, str2, True)\n",
    "            if len(s9) > 1:\n",
    "                s9 += [z for z in s1 if z not in ['er', 'ing', 's', 'less'] and len(z) > 1]\n",
    "            else:\n",
    "                s9.append(word)\n",
    "        else:\n",
    "            s9.append(word)\n",
    "    return (\" \".join(s9))\n",
    "\n",
    "def segmentit(s, txt_arr, t):\n",
    "    st = s\n",
    "    r = []\n",
    "    for j in xrange(len(s)):\n",
    "        for word in txt_arr:\n",
    "            if word == s[:-j]:\n",
    "                r.append(s[:-j])\n",
    "                #print(s[:-j],s[len(s)-j:])\n",
    "                s = s[len(s)-j:]\n",
    "                r += segmentit(s, txt_arr, False)\n",
    "    if t:\n",
    "        i = len((\"\").join(r))\n",
    "        if not i == len(st):\n",
    "            r.append(st[i:])\n",
    "    return r\n",
    "\n",
    "\n",
    "def str_common_word(str1, str2):\n",
    "    words, cnt = str1.split(), 0\n",
    "    for word in words:\n",
    "        if str2.find(word) >= 0:\n",
    "            cnt += 1\n",
    "    return cnt\n",
    "\n",
    "\n",
    "def str_whole_word(str1, str2, i_):\n",
    "    cnt = 0\n",
    "    while i_ < len(str2):\n",
    "        i_ = str2.find(str1, i_)\n",
    "        if i_ == -1:\n",
    "            return cnt\n",
    "        else:\n",
    "            cnt += 1\n",
    "            i_ += len(str1)\n",
    "    return cnt\n",
    "\n",
    "\n",
    "def levenshtein(str1, str2):\n",
    "    if len(str1) < len(str2):\n",
    "        return levenshtein(str2, str1)\n",
    "    if len(str2) == 0:\n",
    "        return len(str1)\n",
    "    previous_row = range(len(str2) + 1)\n",
    "    for i, c1 in enumerate(str1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(str2):\n",
    "            insertions = previous_row[j + 1] + 1 # j+1 instead of j since previous_row and current_row are one character longer\n",
    "            deletions = current_row[j] + 1       # than str2\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    return previous_row[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stemming: 11.48 minutes ---\n"
     ]
    }
   ],
   "source": [
    "# Apply stemmer function to clean features w/ text\n",
    "start_time = time.time()\n",
    "df['search_term'] = df['search_term'].map(lambda x: str_stem(x))\n",
    "df['product_title'] = df['product_title'].map(lambda x: str_stem(x))\n",
    "df['product_description'] = df['product_description'].map(lambda x: str_stem(x))\n",
    "df['brand'] = df['brand'].map(lambda x: str_stem(x))\n",
    "print(\"--- Stemming: %s minutes ---\" % round(((time.time() - start_time)/60), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Prod Info: 0.01 minutes ---\n"
     ]
    }
   ],
   "source": [
    "# Create the product_info feature\n",
    "start_time = time.time()\n",
    "df['product_info'] = df['search_term']+\"|\"+df['product_title'] +\"|\"+df['product_description']\n",
    "print(\"--- Prod Info: %s minutes ---\" % round(((time.time() - start_time)/60), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Levenshtein edit distance: 37.94 minutes ---\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Levenshtein edit distances\n",
    "start_time = time.time()\n",
    "df['leveneditdist_query_in_title'] = df['product_info'].map(lambda x: levenshtein(x.split('|')[1], x.split('|')[0]))\n",
    "df['leveneditdist_query_in_product_description'] = df['product_info'].map(lambda x: levenshtein(x.split('|')[2], x.split('|')[0]))\n",
    "print(\"--- Levenshtein edit distance: %s minutes ---\" % round(((time.time() - start_time)/60), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Len of: 37.99 minutes ---\n"
     ]
    }
   ],
   "source": [
    "# Calculate the length of the text features for later use in ratios\n",
    "df['len_of_query'] = df['search_term'].map(lambda x: len(x.split())).astype(int)\n",
    "df['len_of_title'] = df['product_title'].map(lambda x: len(x.split())).astype(int)\n",
    "df['len_of_description'] = df['product_description'].map(lambda x: len(x.split())).astype(int)\n",
    "df['len_of_brand'] = df['brand'].map(lambda x: len(x.split())).astype(int)\n",
    "print(\"--- Len of: %s minutes ---\" % round(((time.time() - start_time)/60), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Search Term Segment: 0.15 minutes ---\n"
     ]
    }
   ],
   "source": [
    "# Apply seg_words function to clean and prepare search_term feature\n",
    "start_time = time.time()\n",
    "df['search_term'] = df['product_info'].map(lambda x: seg_words(x.split('|')[0], x.split('|')[1]))\n",
    "print(\"--- Search Term Segment: %s minutes ---\" % round(((time.time() - start_time)/60),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Query In: 0.03 minutes ---\n",
      "--- Query Last Word In: 0.03 minutes ---\n",
      "--- Word In: 0.04 minutes ---\n"
     ]
    }
   ],
   "source": [
    "# Apply seg_words function to clean data\n",
    "start_time = time.time()\n",
    "df['query_in_title'] = df['product_info'].map(lambda x: str_whole_word(x.split('|')[0], x.split('|')[1], 0))\n",
    "df['query_in_description'] = df['product_info'].map(lambda x: str_whole_word(x.split('|')[0], x.split('|')[2], 0))\n",
    "print(\"--- Query In: %s minutes ---\" % round(((time.time() - start_time)/60), 2))\n",
    "start_time = time.time()\n",
    "df['query_last_word_in_title'] = df['product_info'].map(lambda x: str_common_word(x.split('|')[0].split(\" \")[-1], x.split('|')[1]))\n",
    "df['query_last_word_in_description'] = df['product_info'].map(lambda x: str_common_word(x.split('|')[0].split(\" \")[-1], x.split('|')[2]))\n",
    "print(\"--- Query Last Word In: %s minutes ---\" % round(((time.time() - start_time)/60), 2))\n",
    "start_time = time.time()\n",
    "df['word_in_title'] = df['product_info'].map(lambda x: str_common_word(x.split('|')[0], x.split('|')[1]))\n",
    "df['word_in_description'] = df['product_info'].map(lambda x: str_common_word(x.split('|')[0], x.split('|')[2]))\n",
    "print(\"--- Word In: %s minutes ---\" % round(((time.time() - start_time)/60), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Export df as a .csv\n",
    "df.to_csv('df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Stemmer\n",
    "#stemmer = SnowballStemmer('english')\n",
    "#def str_stemmer(s):\n",
    "#    return \" \".join([stemmer.stem(word) for word in s.lower().split()])\n",
    "\n",
    "# Stem the features which will be used in training\n",
    "#df['product_title'] = df['product_title'].map(lambda x:str_stemmer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select the relevant columns (product_title, search_term, and relevance) from train df and convert to a list of dictionaries\n",
    "#train_dict = df.ix[:, 'product_title':'relevance'].T.to_dict().values()\n",
    "\n",
    "# Instantiate the vectorizer\n",
    "#vectorizer = DictVectorizer()\n",
    "\n",
    "# Vectorize the train data\n",
    "#train_matrix = vectorizer.fit_transform(train_dict).toarray()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
